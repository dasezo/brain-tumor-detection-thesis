\chapter{Contribution}

\section{Introduction}
\label{sec:contribution-introduction}

In this chapter, we present the core contributions of our work on automated brain tumor detection in magnetic resonance (MR) images. Building upon the BraTS benchmark dataset \cite{Menze2015}, our pipeline integrates a deep learning–based segmentation module with a classical machine learning classifier and culminates in a user‐friendly demo application. The main objectives of this chapter are:
\begin{itemize}
  \item To describe a tailored U-Net–based segmentation pipeline for delineating tumor subregions in MRI slices.
  \item To detail a feature‐extraction and SVM classification scheme that distinguishes high‐grade from low‐grade gliomas using volumetric, intensity, texture, and shape descriptors.
  \item To demonstrate the integration of these modules within an end-to-end application for streamlined inference on new patient data.
\end{itemize}

The remainder of this chapter is organized as follows. In Section~\ref{sec:contribution-dataset}, we introduce the dataset and preprocessing steps. Section~\ref{sec:contribution-segmentation} details the U-Net segmentation module, including architecture and training protocol. Section~\ref{sec:contribution-classification} covers the feature engineering and SVM classification. Section~\ref{sec:contribution-demo} presents the design and functionality of our demo application. We conclude with a discussion of key findings and future directions.

\section{Proposed Framework Overview}
\label{sec:contribution-framework}
In this section, we look at our proposed fremework from a systematic perspective. The framework is designed to perform end-to-end brain tumor segmentation and classification. We will discuss the design of the final pipeline and the training workflow to achieve the desired results.

\subsection{End-to-End Inference Pipeline}
The purpose of our project is to have an end-to-end inference pipeline accepts a raw MR image as input, applies preprocessing steps, performs segmentation of the tumor region using the trained U-Net model, classifies the tumor grade via the SVM classifier, and finally outputs the original image overlaid with the segmentation mask along with the predicted grade.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Images/Chapter3/pipeline.png}
  \caption{Overview of the end-to-end inference pipeline.}
  \label{fig:pipeline}
\end{figure}


\subsection{Model Training Workflow}
The training workflow begins with the BraTS dataset. After preprocessing and augmentation, the data is split into training, validation, and test sets. We then train the U-Net segmentation model in parallel with feature extraction followed by SVM classifier training, yielding two standalone models for inference.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{Images/Chapter3/training.png}
  \caption{Overview of the training workflow.}
  \label{fig:training}
\end{figure}

\section{Dataset and Preprocessing}
\label{sec:contribution-dataset}
In order to train our hybrid model we used the Brain Tumor Segmentation (BraTS) 2020 dataset, which is a collection of multimodal Magnetic Resonance Imaging (MRI) scans used for the segmentation of brain tumors.

\subsection{BraTS Dataset Description}
The dataset includes MRI scans from glioma patients, providing four different MRI modalities per patient:
\begin{enumerate}
  \item \textbf{Native (T1)}
  \item \textbf{Post-contrast T1-weighted (T1ce - contrast enhanced)}
  \item \textbf{T2-weighted (T2)}
  \item \textbf{T2-FLAIR (T2 - Fluid Attenuated Inversion Recovery)}
  \item \textbf{Tumor Segmentation Mask}
\end{enumerate}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Images/Chapter3/modalities.png}
  \caption{Brats modalities: T1, T1ce, T2, and T2-FLAIR.}
  \label{fig:modalities}
\end{figure}

These scans come with expert-annotated segmentation masks that delineate the tumor into various sub-regions, such as the necrotic and non-enhancing tumor core, the peritumoral edema, and the enhancing tumor. Research has demonstrated that accurate segmentation is linked to improved prognostic assessments and treatment outcomes.

\begin{itemize}
  \item \textbf{Class 0 (Not Tumor):} This class represents normal brain tissue or background, where no tumor tissue is present.
  \item \textbf{Class 1 (Non-Enhancing Tumor):} This class corresponds to the necrotic or non-enhancing core regions of the tumor. These areas typically lack contrast enhancement and may include dead or less active tumor tissue.
  \item \textbf{Class 2 (Edema):} This class identifies regions of peritumoral edema, which is the swelling around the tumor caused by fluid accumulation. Edema is important for understanding the extent of the tumor’s impact on surrounding brain tissue.
  \item \textbf{Class 4 (Enhancing Tumor):} This class captures the actively enhancing parts of the tumor, visible after the administration of a contrast agent. These regions often indicate aggressive tumor tissue with increased blood flow and permeability.
\end{itemize}

To visually interpret these segmentations, we map the categorical labels to a custom colormap. In our example, we use four distinct colors to represent:

\begin{figure}[H]
  \centering
  \includegraphics[width=1.1\textwidth]{Images/Chapter3/tclass.png}
  \caption{Segmentation of Tumor classes.}
  \label{fig:tclass}
\end{figure}

\subsection{Dataset Splitting}
To train and evaluate our model effectively, we need to partition our dataset into three subsets:
\begin{itemize}
  \item \textbf{Training Set (70\%):} Used to learn the model parameters.
  \item \textbf{Validation Set (approximately 20\%):} Used for tuning hyperparameters and preventing overfitting.
  \item \textbf{Test Set (10\%):} Used for assessing the final model’s performance on unseen data.
\end{itemize}
This split can be done randomly or in a stratified manner (to preserve the class distribution), which is especially useful when dealing with imbalanced datasets. Properly splitting the dataset is crucial for building a robust model that generalizes well to new data.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Images/Chapter3/data_distribution.png}
  \caption{Distribution of the training, validation, and test sets.}
  \label{fig:data_distribution}
\end{figure}


\subsection{Data Preprocessing}
\label{sec:contribution-preprocessing}

Before feeding MR volumes into our models, we apply a series of standardized preprocessing steps to ensure consistency and improve model robustness. Our pipeline operates on 2D axial slices extracted from 3D volumes, as follows:

\begin{enumerate}
  \item \textbf{Slice Extraction.}
        For each patient volume, we select 100 consecutive axial slices starting at index 22. This avoids initial and final slices that contain little anatomical information.

  \item \textbf{Resizing.}
        \begin{itemize}
          \item \emph{Image Slices:} Each extracted slice is resized to \texttt{128$\times$128} pixels to match the U-Net input dimensions.
          \item \emph{Segmentation Masks:} Corresponding ground-truth masks are first resized to \texttt{240$\times$240} (to preserve label fidelity) and later downsampled alongside images during one-hot encoding.
        \end{itemize}

  \item \textbf{Intensity Normalization.}
        All pixel intensities in a slice are divided by the global maximum value of that volume, scaling inputs to the \([0,1]\) range. This step harmonizes contrast across patients and modalities.

  \item \textbf{Augmentation.}
        To increase effective training diversity, random geometric transformations are applied during batch generation:
        \begin{itemize}
          \item Horizontal and vertical flips (each with 50\% probability).
          \item Rotations by multiples of 90° (randomly chosen among 0°, 90°, 180°, 270°).
        \end{itemize}
\end{enumerate}

These preprocessing routines standardize input dimensions, normalize intensity distributions, and inject variability—laying a solid foundation for both segmentation and classification tasks.

\section{Segmentation Module}
\label{sec:contribution-segmentation}

The segmentation module is responsible for delineating tumor subregions in MR slices. It consists of a U-Net–based convolutional network for mask prediction.


\subsection{U-Net Architecture}
U-Net is a convolutional neural network architecture specifically designed for biomedical image segmentation. Introduced by Ronneberger et al. in 2015, U-Net features a symmetric encoder-decoder structure: the contracting path (encoder) captures image context through successive convolution and pooling operations, while the expansive path (decoder) enables precise localization via upsampling and concatenation with high-resolution features from the encoder. This architecture allows U-Net to achieve accurate segmentation even with limited annotated data by leveraging extensive data augmentation. U-Net has demonstrated superior performance in various biomedical segmentation challenges, notably outperforming previous methods in tasks such as neuronal structure segmentation in electron microscopy images and cell tracking in light microscopy \cite{ronneberger2015u}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Images/Chapter1/unet2.png}
  \caption{U-Net Architecture.}
  \label{fig:unet}
\end{figure}

\subsubsection{Key Components of a U-Net Architecture}

\begin{itemize}
  \item \textbf{Contracting Path (Encoder):} \\
        This path is responsible for extracting contextual features from the input image. It consists of repeated blocks of two $3\times3$ convolutional layers (with ReLU activation), followed by a $2\times2$ max pooling operation for downsampling. With each downsampling step, the number of feature channels is doubled, allowing the network to capture increasingly abstract representations of the input.

  \item \textbf{Bottleneck:} \\
        Located at the deepest part of the network, the bottleneck consists of convolutional layers without pooling. It serves as the bridge between the encoder and decoder, capturing the most condensed and abstract features of the input.

  \item \textbf{Expansive Path (Decoder):} \\
        This path reconstructs the spatial resolution of the feature maps and enables precise localization. Each step in the decoder involves upsampling the feature map (often via transposed convolution or up-convolution), concatenating it with the corresponding feature map from the encoder (skip connection), and then applying two $3\times3$ convolutions (with ReLU activation). The number of feature channels is halved at each upsampling step.

  \item \textbf{Skip Connections:} \\
        At each level, feature maps from the encoder are concatenated with the upsampled feature maps in the decoder. These skip connections help retain high-resolution spatial information that might otherwise be lost during downsampling, improving the accuracy of segmentation boundaries.

  \item \textbf{Final Output Layer:} \\
        The last layer is typically a $1\times1$ convolution that maps each feature vector to the desired number of output classes, producing a pixel-wise classification map for segmentation tasks.
\end{itemize}

This U-shaped design enables U-Net to effectively combine global context with fine-grained localization, making it highly effective for precise image segmentation tasks.
Our U-Net follows the standard encoder–decoder pattern with skip connections:
\begin{itemize}
  \item \textbf{Encoder:} Four downsampling blocks, each with two \texttt{Conv2D} layers (kernel size 3×3, \texttt{ReLU} activation) followed by \texttt{MaxPooling2D}.
  \item \textbf{Bottleneck:} Two \texttt{Conv2D} layers at the lowest resolution, with a dropout of 0.2 to reduce overfitting.
  \item \textbf{Decoder:} Four upsampling blocks, each using \texttt{UpSampling2D} + \texttt{Conv2D (2×2)} and concatenation with the corresponding encoder feature map.
  \item \textbf{Output:} A final \texttt{Conv2D} layer with a 1×1 kernel and \texttt{softmax} activation to produce a four-channel segmentation mask.
\end{itemize}

\subsection{Evaluation Metrics for Segmentation}
\label{sec:segmentation-metrics}

In segmentation tasks, \emph{accuracy} measures the overall proportion of correctly classified pixels. However, in datasets like BraTS2020—where the background (non‐tumor) pixels vastly outnumber tumor pixels—accuracy can be misleading. Therefore, we employ the following metrics for a more balanced evaluation:

\begin{itemize}
  \item \textbf{Precision} (Positive Predictive Value)
        Measures the fraction of predicted tumor pixels that are truly tumor:
        \[
          \text{Precision} \;=\; \frac{\text{TP}}{\text{TP} + \text{FP}}
        \]
        where
        \begin{itemize}
          \item \(\text{TP}\) = number of true positive pixels,
          \item \(\text{FP}\) = number of false positive pixels.
        \end{itemize}

  \item \textbf{Sensitivity} (Recall or True Positive Rate)
        Measures the fraction of actual tumor pixels correctly identified:
        \[
          \text{Sensitivity} \;=\; \frac{\text{TP}}{\text{TP} + \text{FN}}
        \]
        where
        \begin{itemize}
          \item \(\text{FN}\) = number of false negative pixels.
        \end{itemize}

  \item \textbf{Specificity} (True Negative Rate)
        Measures the fraction of non‐tumor pixels correctly classified:
        \[
          \text{Specificity} \;=\; \frac{\text{TN}}{\text{TN} + \text{FP}}
        \]
        where
        \begin{itemize}
          \item \(\text{TN}\) = number of true negative pixels.
        \end{itemize}
  \item \textbf{Intersection over Union (IoU)}
        Also known as the Jaccard index, IoU measures overlap between prediction and ground truth:
        \[
          \text{IoU} \;=\; \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}}.
        \]
        We report the \emph{mean IoU} (mIoU) averaged over the four classes.

  \item \textbf{Dice Coefficient} (F1 Score)
        The Dice coefficient emphasizes overlap and is defined as:
        \[
          \text{Dice} \;=\; \frac{2\,\text{TP}}{2\,\text{TP} + \text{FP} + \text{FN}}.
        \]
        We compute both the \emph{overall Dice} (averaged across classes) and \emph{per‐class Dice} for necrotic/core, edema, and enhancing tissue.
\end{itemize}

\subsection{Segmentation Results}
\label{sec:segmentation-results}
In this section, we present the results of our U-Net segmentation model on the BraTS2020 dataset. The model was trained for 50 epochs with a batch size of 16, we will discuss the end results of the training and validation process, including loss and accuracy metrics.

\subsubsection{Accuracy}
The model achieved an impressive pixel-level accuracy of 99.3\%, indicating that the vast majority of pixels were correctly classified. This high accuracy is particularly important in medical imaging, where even small errors can have significant implications.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{Images/Chapter3/unet_acc.png}
  \caption{Segmentation accuracy over epochs.}
  \label{fig:unet-acc}
\end{figure}

\subsubsection{Loss}
The loss function used during training was the categorical cross-entropy loss, which measures the dissimilarity between the predicted and true distributions. The model converged to a low loss of 0.0231, indicating that the predictions were closely aligned with the ground truth.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{Images/Chapter3/unet_loss.png}
  \caption{Segmentation loss over epochs.}
  \label{fig:unet-loss}
\end{figure}

\subsubsection{Dice Coefficient}
The Dice coefficient is a measure of overlap between the predicted and true segmentation masks. The overall Dice coefficient achieved was 58.98\%, indicating a good level of agreement between the predicted and true tumor regions. The per-class Dice coefficients were also calculated, providing insights into the model's performance on different tumor subregions.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{Images/Chapter3/unet_dice.png}
  \caption{Segmentation Dice coefficient over epochs.}
  \label{fig:unet-dice}
\end{figure}

\subsubsection{Mean IoU}
The mean Intersection over Union (IoU) was calculated to assess the model's performance across all classes. The mean IoU achieved was 74.66\%, indicating a good level of overlap between the predicted and true segmentation masks. This metric is particularly useful in medical imaging, where accurate delineation of tumor boundaries is crucial for treatment planning.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{Images/Chapter3/unet_iou.png}
  \caption{Segmentation mean IoU over epochs.}
  \label{fig:unet-iou}
\end{figure}

Table~\ref{tab:segmentation-results} summarizes the quantitative performance, and Figure~\ref{fig:segmentation-example} shows a representative qualitative result.
\begin{table}[ht]
  \centering
  \caption{Segmentation performance on the test set}
  \label{tab:segmentation-results}
  \begin{tabular}{l r}
    \hline
    \textbf{Metric}            & \textbf{Value} \\
    \hline
    Loss                       & 0.0231         \\
    Accuracy                   & 99.30\,\%      \\
    Mean IoU                   & 74.66\,\%      \\
    Dice Coefficient (overall) & 58.98\,\%      \\
    Precision                  & 99.37\,\%      \\
    Sensitivity                & 99.08\,\%      \\
    Specificity                & 99.79\,\%      \\
    \hline
  \end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{Images/Chapter3/seg.png}
  \caption{Example of segmentation results.}
  \label{fig:segmentation-example}
\end{figure}




Overall, the model converged to a low loss (0.0231) and achieved excellent pixel‐level accuracy (99.3 \%), demonstrating strong background discrimination (specificity = 99.8 \%). The mean IoU of 74.7 \% and overall Dice of 59.0 \% indicate reliable overlap between prediction and ground truth also confirms that tumor regions are both accurately and comprehensively detected.


\section{Classification Module}
\label{sec:contribution-classification}

The classification module distinguishes high‐grade gliomas (HGG) from low‐grade gliomas (LGG) using handcrafted features extracted from the segmented tumor regions and a support vector machine (SVM) classifier.
\subsection{Support Vector Machines (SVM)}
\label{sec:svm}
Support Vector Machines (SVMs) are supervised machine learning models widely used for classification and regression tasks. The core idea of SVM is to find an optimal hyperplane that separates data points of different classes with the maximum possible margin, which enhances the model’s ability to generalize to unseen data. SVMs can efficiently handle both linear and non-linear classification problems by employing the kernel trick, which implicitly maps input data into a higher-dimensional feature space where a linear separation becomes possible. The theoretical foundation of SVMs is based on the Structural Risk Minimization (SRM) principle, which aims to minimize an upper bound on the generalization error, offering advantages over traditional Empirical Risk Minimization approaches. Originally developed by Vapnik and colleagues in the 1990s, SVMs have become popular due to their strong empirical performance and robustness to overfitting, especially in high-dimensional spaces \cite{Gunn1998SupportVM}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{Images/Chapter1/svm.png}
  \caption{Support Vector Machine.}
  \label{fig:svm}
\end{figure}
The fundamental formula defining the decision boundary of a Support Vector Machine (SVM) is a hyperplane expressed as:
\begin{equation}
  \mathbf{w}^\top \mathbf{x} + b = 0
\end{equation}
where $\mathbf{w}$ is the weight vector normal to the hyperplane, $\mathbf{x}$ is the input feature vector, and $b$ is the bias term.

For binary classification with labels $y_i \in \{+1, -1\}$, the SVM enforces the following constraints on each training point $(\mathbf{x}_i, y_i)$:
\begin{equation}
  y_i \bigl(\mathbf{w}^\top \mathbf{x}_i + b\bigr) \;\ge\; 1,
  \quad \forall\,i.
\end{equation}

The margin width (the distance between the closest points of each class to the hyperplane) is given by $\tfrac{2}{\|\mathbf{w}\|_2}$.  Maximizing this margin is therefore equivalent to minimizing $\|\mathbf{w}\|_2$, leading to the following convex optimization problem:

\begin{align}
  \min_{\mathbf{w},\,b} \quad & \frac{1}{2} \|\mathbf{w}\|_2^2,                             \\
  \text{subject to} \quad     & y_i \bigl(\mathbf{w}^\top \mathbf{x}_i + b\bigr) \;\ge\; 1,
  \quad \forall\,i.
\end{align}

For non-linearly separable data, slack variables and kernel functions can be introduced, but the core formulation remains centered on maximizing the margin around this hyperplane.

\subsection{Feature Extraction}
From each segmented case, we compute the following feature categories:
\begin{itemize}
  \item \textbf{Volume Features:} Volumes of necrotic/core (NCR), edema (ED), enhancing tumor (ET), tumor core (TC), and whole tumor (WT), plus their ratios (e.g., TC/WT, ET/TC).
  \item \textbf{Intensity Statistics:} Mean, standard deviation, minimum, maximum, median, 10th/90th percentiles, and range of voxel intensities for each modality (FLAIR, T1, T1CE, T2) within each tumor component.
  \item \textbf{Texture Features:} Histogram of oriented gradients (HOG)–based descriptors computed on each component.
  \item \textbf{Shape Features:} Extents along each axis, elongation, flatness, and sphericity of the whole tumor mask.
  \item \textbf{Heterogeneity Features:} Contrast between core and edema, and between enhancing and necrotic regions for each modality.
\end{itemize}

\subsection{Feature Selection}
\begin{itemize}
  \item Fit a Random Forest classifier on the training split to compute feature importances.
  \item Select the top 30 most important features for downstream classification.
\end{itemize}

\subsection{SVM Model Training}
\begin{itemize}
  \item \textbf{Data Split:} 80\% training, 20\% test (stratified by grade).
  \item \textbf{Scaling:} Standardize features to zero mean and unit variance.
  \item \textbf{Hyperparameter Search:} Grid search over
        \(\{C \in [0.1,1,10,100]\}\), \(\gamma \in \{\mathrm{scale},\mathrm{auto},0.01,0.1\}\), and \(\{\kern-0.1em\mathrm{rbf},\mathrm{linear},\mathrm{poly}\}\) kernels, using 5-fold CV.
  \item \textbf{Final Model:} Best‐estimator SVM retrained on full training set.
\end{itemize}

\subsection{Evaluation Metrics for Classification}
We assess performance on the held‐out test set using:
\begin{itemize}
  \item \textbf{Accuracy:} Fraction of correctly classified patients.
        \[
          \text{Accuracy} \;=\; \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \]
  \item \textbf{Precision, Recall, F1-Score:} Computed per class and averaged.
        \[
          \text{Precision} \;=\; \frac{\text{TP}}{\text{TP} + \text{FP}}, \qquad
          \text{Recall} \;=\; \frac{\text{TP}}{\text{TP} + \text{FN}}, \qquad
          \text{F1} \;=\; \frac{2 \cdot \text{TP}}{2 \cdot \text{TP} + \text{FP} + \text{FN}}
        \]
  \item \textbf{Confusion Matrix:} Counts of true vs.\ predicted labels.
  \item \textbf{ROC AUC:} Area under the receiver operating characteristic curve.
        \[
          \text{AUC} \;=\; \frac{1}{2} \sum_{i=1}^{n} (\text{TPR}_{i} + \text{TPR}_{i-1}) \cdot (\text{FPR}_{i} - \text{FPR}_{i-1})
        \]
        where $n$ is the number of thresholds and $\text{TPR}_i$ and $\text{FPR}_i$ are the true positive and false positive rates at the $i$th threshold.
\end{itemize}

\subsection{Classification Results}
\label{sec:classification-results}

The SVM classifier was optimized via grid search, yielding the following best hyperparameters:
\begin{itemize}
  \item \(C = 1\), \(\gamma = \text{scale}\), Kernel = \texttt{linear}
\end{itemize}
On the held‐out test set (74 patients), the model achieved an overall accuracy of 93.24\,\%.


\begin{table}[ht]
  \centering
  \caption{Classification report}
  \label{tab:svm-report}
  \begin{tabular}{lcccc}
    \hline
    Class        & Precision                     & Recall & F1-Score & Support \\
    \hline
    LGG (0)      & 86\,\%                        & 80\,\% & 83\,\%   & 15      \\
    HGG (1)      & 95\,\%                        & 97\,\% & 96\,\%   & 59      \\
    \hline
    Accuracy     & \multicolumn{4}{c}{93.24\,\%}                               \\
    Macro avg    & 90\,\%                        & 88\,\% & 89\,\%   & 74      \\
    Weighted avg & 93\,\%                        & 93\,\% & 93\,\%   & 74      \\
    \hline
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{Images/Chapter3/confusion.png}
  \caption{Confusion matrix for the SVM classifier.}
  \label{fig:confusion}
\end{figure}

\paragraph{Example Inference}
On a new patient (ID 083), the pipeline predicted:
\begin{itemize}
  \item \textbf{Prediction:} HGG
  \item \textbf{Probability of HGG:} 93.60\,\%
  \item \textbf{Actual Grade:} HGG (correct)
\end{itemize}

\section{Application Demo}
\label{sec:contribution-demo}

To illustrate end‐user interaction, we developed a lightweight demo application that integrates our trained U-Net and SVM models into a single GUI. The application  consists of two main pages:

\subsection{Upload Page}
\label{sec:contribution-demo-upload}

Presents an HTML form where the user can select and upload a brain tumor image (2D slice).
Upon submission, the form sends a POST request to the \texttt{/results} route.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Images/Chapter3/app_interface.png}
  \caption{Upload page of the application demo.}
  \label{fig:demo-upload}
\end{figure}

\subsection{Results Page}
\label{sec:contribution-demo-results}

Receives the uploaded image, runs the preprocessing, segmentation (U-Net), feature extraction, and classification (SVM) pipeline, and then renders:
\begin{itemize}
  \item The original input image.
  \item The segmentation mask overlaid on the input.
  \item The predicted tumor grade (LGG/HGG) with its confidence score.
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Images/Chapter3/app_result.png}
  \caption{Results page of the application demo.}
  \label{fig:demo-results}
\end{figure}

\section{Conclusion}
\label{sec:contribution-conclusion}

In this chapter, we have presented a comprehensive methodology for automated brain tumor detection and classification using MR images. Our approach integrates a U-Net-based segmentation model with an SVM classifier, achieving high accuracy and robust performance across multiple evaluation metrics. The segmentation module demonstrated reliable delineation of tumor subregions, while the classification module effectively distinguished between high-grade and low-grade gliomas. Additionally, we showcased the practical application of our framework through a user-friendly demo application, highlighting its potential for real-world clinical use. These contributions underscore the effectiveness of combining deep learning and classical machine learning techniques in addressing complex medical imaging challenges. Future work could explore further optimization of the pipeline, incorporation of additional data modalities, and validation on larger, more diverse datasets.



